{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "497f9ebd",
   "metadata": {},
   "source": [
    "<div style=\"font-family:Georgia; line-height:2.0; font-size:18px;\">\n",
    "\n",
    "<p align=\"center\" style=\"font-size:26px; font-weight:bold;\">Lesson on Using Voyant for Text Analysis</p>\n",
    "\n",
    "<p style=\"text-decoration:underline; font-weight:bold;\">Introduction:</p>\n",
    "We can approach analyzing text in a digital humanities context through quantitative methods, rather than the qualitative ones literary scholars traditionally use. There are many such approaches to this, but this lesson will focus on Voyant, a text analysis website. In this lesson, we will explore its features and what insights can be drawn using text analysis broadly that may not otherwise be possible through personal interpretation.\n",
    "\n",
    "<p style=\"text-decoration:underline; font-weight:bold;\">Lesson Goals:</p>\n",
    "Understand the following features that Voyant provides us and how they're made/calculated:\n",
    "<ul>\n",
    "  <li>Contexts</li>\n",
    "  <li>Collocates</li>\n",
    "  <li>Vocabulary density</li>\n",
    "  <li>Readability index</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-decoration:underline; font-weight:bold;\">Getting Started:</p>\n",
    "Please visit this link: [https://voyant-tools.org/?corpus=820e0b4f941cae962a54ec2ddf611dee](https://voyant-tools.org/?corpus=820e0b4f941cae962a54ec2ddf611dee). It is a link to Voyant with a corpus already provided. For this lesson, we will use the entire Harry Potter series (as that was what was easily available online). Of course, know that Voyant lets you use any other link/pdf you would like.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2337ef0",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"Screenshot 2025-04-26 at 10.45.46 PM.png\" width=\"800\">\n",
    "</p>\n",
    "\n",
    "<p align=\"center\"><em>What Voyant looks like</em></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a1afd",
   "metadata": {},
   "source": [
    "<div style=\"font-family:Georgia; line-height:2.0; font-size:18px;\">\n",
    "\n",
    "The image above is the first thing that pops up. The entire screen is split up into five sections with each section having multiple types of information to display if you click on that section's different options (e.g., \"Contexts\" and \"Collocates\" in the bottom right). Most obviously we see the words in the top left which show the relative frequency of words by comparison of size. Some of the biggest (and thus most frequent) words there are \"and,\" \"the,\" \"to,\" and \"harry\" (as we would expect). If you hover over a certain word, you can get exactly how many times it was used — Harry was said 15,713 times.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Screenshot 2025-04-26 at 11.05.45 PM.png\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\" style=\"font-size:14px; color:gray;\"><em>Contexts</em></p>\n",
    "\n",
    "Our first term: **Contexts** (above). Contexts selects a certain word, and scrapes the words preceding and proceeding it — in other words, the words on its left and right. We can change how many words we include through the lever at the very bottom of that section, if we decide that we want more contextual information about a certain word. Conceptually, we can code \"contexts\" by finding the index of our target word, and then using left and right trim to cut off our desired number of words.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Screenshot 2025-04-26 at 11.04.15 PM.png\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\" style=\"font-size:14px; color:gray;\"><em>Collocates</em></p>\n",
    "\n",
    "Our second term: **Collocates** (above). Also located in the bottom right, Collocates, for a certain target word, analyzes what other words frequently appear around it. Again, you can modify the bounds of the search to give more specificity to \"being around.\" For the word \"Harry,\" we see that the most frequent collocates are \"the,\" \"and,\" and \"to.\" This is to be expected and doesn't particularly say much. But other collocates — where \"Ron\" is used three hundred more times than \"Hermione,\" and nine hundred more times than \"Potter\" — are a little more interesting. (One hypothesis we could form from this is: does Ron appear more often or have more scenes than Hermione?)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Screenshot 2025-04-26 at 11.19.51 PM.png\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\" style=\"font-size:14px; color:gray;\"><em>Readability Index and Vocabulary Density</em></p>\n",
    "\n",
    "Our third and fourth terms: **Readability Index** and **Vocabulary Density** (above). \n",
    "Readability Index is a measure of the reading difficulty/feasibility of a certain text. There are many ways of calculating this metric, but in general, all methods look at how many words are used per sentence, how many syllables per word, the ratio of \"complex\" words to \"normal\" words, etc. In this case, using the Flesch Reading Scale, Harry Potter scores a \"standard\" difficulty with a rating of ~65. For reference, 20 to 50 is considered college level.  \n",
    "Vocabulary Density is a measure of how diverse/rich a text's vocabulary is. It's calculated by doing: Number of Unique Words ÷ Total Number of Words. For Harry Potter, we see it has a score of ~0.05. This actually is incredibly low and I think it's off. A young-adult novel like this one typically sits around ~0.20, which brings me to the point that:  \n",
    "\n",
    "These text analysis websites aren't perfect and still have lots of faults, and this is one of many examples. This is where the human aspect of digital humanities comes into play, where our subjective interpretations are something that can't be quantified like word frequency. Another fault could be from the text itself too, if it has typos. Voyant won't be able to recognize a missing space between two words, for example, and will just combine them into one word that doesn't actually exist. For example, this picture below is what I came across when using the search feature for \"harry\":\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Screenshot 2025-04-26 at 11.28.23 PM.png\" width=\"500\">\n",
    "</p>\n",
    "<p align=\"center\" style=\"font-size:14px; color:gray;\"><em>One of the, frankly, inevitable faults of text analysis sites like Voyant</em></p>\n",
    "\n",
    "Obviously, the words \"harrysaw\" and \"harryand\" don't exist but we can clearly see this is just a matter of a missing space typo.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaaf828",
   "metadata": {},
   "source": [
    "<div style=\"font-family:Georgia; line-height:2.0; font-size:18px;\">\n",
    "\n",
    "\n",
    "<p style=\"text-decoration:underline; font-weight:bold;\">Conclusion:</p>\n",
    "\n",
    "In this lesson, we learned about a couple of the metrics and features of Voyant. We explored how they're calculated, what they mean, and where they can sometimes fail.\n",
    "\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
